{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒ³ ðŸ”„ Complete Guide to Data Transformation A to Z\n",
    "\n",
    "Welcome to this comprehensive guide on data transformation, designed to equip you with the knowledge and skills to effectively preprocess and transform your datasets. Whether you're a budding data scientist or a seasoned professional looking to refine your data transformation techniques, this notebook is tailored for you!\n",
    "\n",
    "## What Will You Learn?\n",
    "\n",
    "In this guide, we will explore various methods to normalize, construct, discretize, and aggregate features, ensuring you have the tools to confidently prepare your data for any analysis or modeling task. Here's what we'll cover:\n",
    "\n",
    "### 1. Feature Normalization\n",
    "\n",
    "Learn how to scale and adjust the statistical distribution of feature values to improve the performance and accuracy of your models.\n",
    "\n",
    "- **Min-Max Scaling**: Scale features to a fixed range, typically 0 to 1.\n",
    "- **Z-Score Standardization**: Transform features to have a mean of 0 and a standard deviation of 1.\n",
    "- **Robust Scaling**: Scale features using statistics that are robust to outliers, such as the median and interquartile range.\n",
    "- **Yeo-Johnson Transformation**: Apply a transformation that can handle both positive and negative values to achieve normality.\n",
    "- **Box-Cox Transformation**: Apply a transformation that works with positive values to achieve normality and reduce skewness.\n",
    "\n",
    "### 2. Feature Construction\n",
    "\n",
    "Learn techniques to create new features from existing ones, enhancing the predictive power of your models.\n",
    "\n",
    "- **Use of Domain Knowledge**: Incorporate insights from the specific field or industry to construct meaningful features.\n",
    "- **Using Statistical Relationships Between Features**: Identify and utilize correlations and interactions between features.\n",
    "- **Numerical Coding of Nominal Values**:\n",
    "  - **One-Hot Encoding**: Convert categorical variables into a series of binary variables.\n",
    "  - **Ordinal or Label Encoding**: Assign integer values to categories based on their order or labels.\n",
    "  - **Probability Ratio Encoding**: Encode categorical features based on the probability ratio of the target variable.\n",
    "\n",
    "### 3. Feature Discretization\n",
    "\n",
    "Learn how to transform continuous features into discrete ones to simplify models and capture nonlinear relationships.\n",
    "\n",
    "- **Domain Knowledge**: Use expert knowledge to define meaningful bins.\n",
    "- **Unsupervised Methods**:\n",
    "  - **Equal-Width Binning**: Divide the range of values into equal-width bins.\n",
    "  - **Equal-Frequency Binning**: Divide the range of values so that each bin has approximately the same number of observations.\n",
    "  - **K-Means Binning**: Use k-means clustering to create bins based on feature similarity.\n",
    "- **Supervised Methods**:\n",
    "  - **ChiMerge**: Merge bins based on the chi-squared statistic to ensure similarity with respect to the target variable.\n",
    "  - **Decision Tree Binning**: Use decision trees to create bins based on target variable splits.\n",
    "\n",
    "\n",
    "## Why This Guide?\n",
    "\n",
    "- **Step-by-Step Tutorials**: Each section includes clear explanations followed by practical examples, ensuring you not only learn but also apply your knowledge.\n",
    "- **Interactive Learning**: Engage with interactive code cells that allow you to see the effects of data transformation methods in real-time.\n",
    "\n",
    "### How to Use This Notebook\n",
    "\n",
    "- **Run the Cells**: Follow along with the code examples by running the cells yourself. Modify the parameters to see how the results change.\n",
    "- **Explore Further**: After completing the guided sections, try applying the methods to your own datasets to reinforce your learning.\n",
    "\n",
    "Prepare to unlock the full potential of data transformation in data analysis. Let's dive in and transform data into valuable insights!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:28:47.459468Z",
     "iopub.status.busy": "2024-05-31T09:28:47.459076Z",
     "iopub.status.idle": "2024-05-31T09:28:47.900567Z",
     "shell.execute_reply": "2024-05-31T09:28:47.899286Z",
     "shell.execute_reply.started": "2024-05-31T09:28:47.459436Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/kaggle/input/loans-and-liability/LoanData_Preprocessed_v1.2.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'ed' and 'default' columns to object type\n",
    "data['ed'] = data['ed'].astype('object')\n",
    "data['default'] = data['default'].astype('object')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:28:47.903554Z",
     "iopub.status.busy": "2024-05-31T09:28:47.903046Z",
     "iopub.status.idle": "2024-05-31T09:28:47.938885Z",
     "shell.execute_reply": "2024-05-31T09:28:47.937752Z",
     "shell.execute_reply.started": "2024-05-31T09:28:47.903508Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Overview\n",
    "\n",
    "The dataset contains information about loan applicants and includes the following columns:\n",
    "\n",
    "- **age**: The age of the applicant, indicating how many years they have lived.\n",
    "  - **Range**: 18 - 66\n",
    "  - **Mean**: 34.40\n",
    "  - **Skewness**: Slightly skewed to the right (positive skew).\n",
    "\n",
    "\n",
    "- **employ**: The number of years the applicant has been employed, which can indicate their job stability and experience.\n",
    "  - **Range**: 0 - 31\n",
    "  - **Mean**: 8.21\n",
    "  - **Skewness**: Right-skewed (positive skew).\n",
    "\n",
    "\n",
    "- **address**: The number of years the applicant has lived at their current address, providing insights into their residential stability.\n",
    "  - **Range**: 0 - 28\n",
    "  - **Mean**: 5.58\n",
    "  - **Skewness**: Right-skewed (positive skew).\n",
    "\n",
    "\n",
    "- **income**: The annual income of the applicant (in thousands), representing their earning capacity.\n",
    "  - **Range**: 10 - 330\n",
    "  - **Mean**: 55.50\n",
    "  - **Skewness**: Right-skewed (positive skew).\n",
    "\n",
    "\n",
    "- **debtinc**: The debt-to-income ratio of the applicant, calculated as the percentage of their income that goes towards paying debts. This ratio helps assess their financial burden.\n",
    "  - **Range**: 0.00 - 37.30\n",
    "  - **Mean**: 10.27\n",
    "  - **Skewness**: Right-skewed (positive skew).\n",
    "\n",
    "\n",
    "- **creddebt**: The amount of credit card debt the applicant has (in thousands), showing their reliance on credit and their debt levels.\n",
    "  - **Range**: 0.00 - 22.12\n",
    "  - **Mean**: 3.51\n",
    "  - **Skewness**: Right-skewed (positive skew).\n",
    "\n",
    "\n",
    "- **othdebt**: The amount of other debt the applicant has (in thousands), which includes all other forms of debt apart from credit card debt.\n",
    "  - **Range**: 0.00 - 57.03\n",
    "  - **Mean**: 5.05\n",
    "  - **Skewness**: Right-skewed (positive skew).\n",
    "\n",
    "\n",
    "- **ed**: The education level of the applicant (encoded numerically), where higher numbers may represent higher levels of education.\n",
    "  - **Unique Values**: 1.0, 2.0, 3.0, 4.0, 5.0\n",
    "  - **Most Frequent Value (Mode)**: 1.0\n",
    "\n",
    "\n",
    "- **default**: A binary indicator of whether the applicant defaulted on the loan (1 for default, 0 for no default), indicating their credit risk.\n",
    "  - **Unique Values**: 0, 1\n",
    "  - **Most Frequent Value (Mode)**: 0 (majority did not default)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature Normalization\n",
    "\n",
    "Feature normalization is a crucial step in the data preprocessing pipeline. It involves adjusting the values of numerical features to ensure they have a common scale, which can improve the performance and training stability of machine learning models. In this section, we will explore various normalization techniques and demonstrate how to apply them using practical examples.\n",
    "\n",
    "### Why Normalize Features?\n",
    "\n",
    "Normalization can help in:\n",
    "- **Improving Model Performance**: Algorithms such as gradient descent converge faster with normalized data.\n",
    "- **Enhancing Accuracy**: Normalization can reduce the impact of features with larger scales on the model.\n",
    "- **Stability**: Models can become more stable and less sensitive to variations in the data.\n",
    "\n",
    "### Techniques Covered:\n",
    "\n",
    "1. **Min-Max Scaling**: This technique scales the features to a fixed range, typically [0, 1]. The formula is:\n",
    "\n",
    "   $$\n",
    "   X' = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
    "   $$\n",
    "\n",
    "2. **Z-Score Standardization**: Also known as standardization, this method transforms features to have a mean of 0 and a standard deviation of 1. The formula is:\n",
    "\n",
    "   $$\n",
    "   X' = \\frac{X - \\mu}{\\sigma}\n",
    "   $$\n",
    "\n",
    "   where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation of the feature.\n",
    "\n",
    "3. **Robust Scaling**: This method scales features using statistics that are robust to outliers, such as the median and the interquartile range. The formula is:\n",
    "\n",
    "   $$\n",
    "   X' = \\frac{X - Q2}{Q3 - Q1}\n",
    "   $$\n",
    "\n",
    "   where \\(Q1\\) and \\(Q3\\) are the 1st and 3rd quartiles, respectively, and \\(Q2\\) is the median.\n",
    "\n",
    "4. **Yeo-Johnson Transformation**: This technique can handle both positive and negative values and transforms the data to be more normally distributed.\n",
    "\n",
    "5. **Box-Cox Transformation**: This method works with positive values and transforms the data to be more normally distributed, reducing skewness.\n",
    "\n",
    "### Why Split Train and Test Data?\n",
    "\n",
    "Splitting the data into training and testing sets is a crucial step in the machine learning pipeline. It ensures that the model's performance can be evaluated on unseen data, providing a more realistic estimate of its effectiveness in real-world scenarios. By keeping the test data separate:\n",
    "- **Avoid Data Leakage**: Ensures that information from the test set does not influence the model during training.\n",
    "- **Model Evaluation**: Provides an unbiased evaluation metric for how well the model generalizes to new data.\n",
    "- **Hyperparameter Tuning**: Helps in tuning model parameters by validating performance on the test set.\n",
    "\n",
    "Let's see how to apply these transformations to our dataset after splitting the data into training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:28:47.940147Z",
     "iopub.status.busy": "2024-05-31T09:28:47.939851Z",
     "iopub.status.idle": "2024-05-31T09:28:48.451637Z",
     "shell.execute_reply": "2024-05-31T09:28:48.45011Z",
     "shell.execute_reply.started": "2024-05-31T09:28:47.940122Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_data, test_data = train_test_split(data.copy(), test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Min-Max Scaling\n",
    "\n",
    "Min-Max Scaling transforms features by scaling them to a given range, usually [0, 1]. This technique is useful when the features have different ranges and you want to ensure they contribute equally to the analysis.\n",
    "\n",
    "### Columns\n",
    "\n",
    "For our dataset, the following columns are suitable for Min-Max Scaling:\n",
    "- `age`\n",
    "- `debtinc`\n",
    "- `creddebt`\n",
    "\n",
    "### Applying Min-Max Scaling\n",
    "\n",
    "Let's apply Min-Max Scaling to these columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:28:48.453956Z",
     "iopub.status.busy": "2024-05-31T09:28:48.453449Z",
     "iopub.status.idle": "2024-05-31T09:28:48.491145Z",
     "shell.execute_reply": "2024-05-31T09:28:48.489873Z",
     "shell.execute_reply.started": "2024-05-31T09:28:48.45391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Select the columns to scale\n",
    "columns_to_scale = ['age', 'debtinc', 'creddebt']\n",
    "\n",
    "# Fit the scaler to the training data and transform both training and testing data\n",
    "train_data_min_max_scaled = train_data.copy()\n",
    "test_data_min_max_scaled = test_data.copy()\n",
    "\n",
    "train_data_min_max_scaled[columns_to_scale] = scaler.fit_transform(train_data[columns_to_scale])\n",
    "test_data_min_max_scaled[columns_to_scale] = scaler.transform(test_data[columns_to_scale])\n",
    "\n",
    "# Display the first few rows of the transformed training dataset to verify the scaling\n",
    "print(\"Min-Max Scaled Data (Train):\")\n",
    "display(train_data_min_max_scaled.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Z-Score Standardization\n",
    "\n",
    "Z-Score Standardization, also known as standardization, transforms features to have a mean of 0 and a standard deviation of 1. This method is less sensitive to outliers and ensures that each feature contributes equally to the analysis.\n",
    "\n",
    "### Columns\n",
    "\n",
    "For our dataset, the following columns are suitable for Z-Score Standardization:\n",
    "- `age`\n",
    "- `debtinc`\n",
    "- `creddebt`\n",
    "\n",
    "### Applying Z-Score Standardization\n",
    "\n",
    "Let's apply Z-Score Standardization to these columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:28:48.495577Z",
     "iopub.status.busy": "2024-05-31T09:28:48.495181Z",
     "iopub.status.idle": "2024-05-31T09:28:48.529006Z",
     "shell.execute_reply": "2024-05-31T09:28:48.52775Z",
     "shell.execute_reply.started": "2024-05-31T09:28:48.495546Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Select the columns to scale\n",
    "columns_to_scale = ['age', 'debtinc', 'creddebt']\n",
    "\n",
    "# Fit the scaler to the training data and transform both training and testing data\n",
    "train_data_z_score_scaled = train_data.copy()\n",
    "test_data_z_score_scaled = test_data.copy()\n",
    "\n",
    "train_data_z_score_scaled[columns_to_scale] = scaler.fit_transform(train_data[columns_to_scale])\n",
    "test_data_z_score_scaled[columns_to_scale] = scaler.transform(test_data[columns_to_scale])\n",
    "\n",
    "# Display the first few rows of the transformed training dataset to verify the scaling\n",
    "print(\"Z-Score Standardized Data (Train):\")\n",
    "display(train_data_z_score_scaled.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Robust Scaling\n",
    "\n",
    "Robust Scaling uses statistics that are robust to outliers, such as the median and the interquartile range, to scale features. This method is useful when the dataset contains outliers that could skew the results of standard scaling methods.\n",
    "\n",
    "### Columns\n",
    "\n",
    "For our dataset, the following columns are suitable for Robust Scaling:\n",
    "- `income`\n",
    "- `othdebt`\n",
    "\n",
    "### Applying Robust Scaling\n",
    "\n",
    "Let's apply Robust Scaling to these columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:28:48.531344Z",
     "iopub.status.busy": "2024-05-31T09:28:48.53062Z",
     "iopub.status.idle": "2024-05-31T09:28:48.564547Z",
     "shell.execute_reply": "2024-05-31T09:28:48.563505Z",
     "shell.execute_reply.started": "2024-05-31T09:28:48.531303Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Initialize the RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Select the columns to scale\n",
    "columns_to_scale = ['income', 'othdebt']\n",
    "\n",
    "# Fit the scaler to the training data and transform both training and testing data\n",
    "train_data_robust_scaled = train_data.copy()\n",
    "test_data_robust_scaled = test_data.copy()\n",
    "\n",
    "train_data_robust_scaled[columns_to_scale] = scaler.fit_transform(train_data[columns_to_scale])\n",
    "test_data_robust_scaled[columns_to_scale] = scaler.transform(test_data[columns_to_scale])\n",
    "\n",
    "# Display the first few rows of the transformed training dataset to verify the scaling\n",
    "print(\"Robust Scaled Data (Train):\")\n",
    "display(train_data_robust_scaled.head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Yeo-Johnson and Box-Cox Transformations\n",
    "\n",
    "The Yeo-Johnson and Box-Cox transformations are power transformations used to stabilize variance and make the data more normally distributed. Yeo-Johnson can handle both positive and negative values, whereas Box-Cox is only applicable to positive values.\n",
    "\n",
    "### Columns\n",
    "\n",
    "For our dataset, we will check the following columns for negative values and apply the appropriate transformation:\n",
    "- `age`\n",
    "- `income`\n",
    "- `debtinc`\n",
    "- `creddebt`\n",
    "- `othdebt`\n",
    "\n",
    "### Applying Transformations\n",
    "\n",
    "Let's check for negative values in the columns and apply the Yeo-Johnson Transformation to columns with negative values and the Box-Cox Transformation to columns with only positive values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:28:48.56614Z",
     "iopub.status.busy": "2024-05-31T09:28:48.565814Z",
     "iopub.status.idle": "2024-05-31T09:28:48.610954Z",
     "shell.execute_reply": "2024-05-31T09:28:48.60961Z",
     "shell.execute_reply.started": "2024-05-31T09:28:48.566112Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Check if any column contains negative values\n",
    "columns_to_check = ['age', 'income', 'debtinc', 'creddebt', 'othdebt']\n",
    "contains_negative = {col: np.any(train_data[col] < 0) for col in columns_to_check}\n",
    "\n",
    "# Initialize the PowerTransformers\n",
    "yeo_johnson_transformer = PowerTransformer(method='yeo-johnson')\n",
    "box_cox_transformer = PowerTransformer(method='box-cox')\n",
    "\n",
    "# Make copies of the train and test data for transformations\n",
    "train_data_yeo_johnson = train_data.copy()\n",
    "test_data_yeo_johnson = test_data.copy()\n",
    "train_data_box_cox = train_data.copy()\n",
    "test_data_box_cox = test_data.copy()\n",
    "\n",
    "# Apply Yeo-Johnson transformation to columns with negative values\n",
    "columns_to_transform_yeo_johnson = [col for col, has_negative in contains_negative.items() if has_negative]\n",
    "if columns_to_transform_yeo_johnson:\n",
    "    train_data_yeo_johnson[columns_to_transform_yeo_johnson] = yeo_johnson_transformer.fit_transform(train_data[columns_to_transform_yeo_johnson])\n",
    "    test_data_yeo_johnson[columns_to_transform_yeo_johnson] = yeo_johnson_transformer.transform(test_data[columns_to_transform_yeo_johnson])\n",
    "    print(\"Yeo-Johnson Transformed Data (Train):\")\n",
    "    display(train_data_yeo_johnson.head(20))\n",
    "else:\n",
    "    print(\"No columns with negative values for Yeo-Johnson transformation.\")\n",
    "\n",
    "# Apply Box-Cox transformation to columns with only positive values\n",
    "columns_to_transform_box_cox = [col for col, has_negative in contains_negative.items() if not has_negative]\n",
    "if columns_to_transform_box_cox:\n",
    "    train_data_box_cox[columns_to_transform_box_cox] = box_cox_transformer.fit_transform(train_data[columns_to_transform_box_cox])\n",
    "    test_data_box_cox[columns_to_transform_box_cox] = box_cox_transformer.transform(test_data[columns_to_transform_box_cox])\n",
    "    print(\"Box-Cox Transformed Data (Train):\")\n",
    "    display(train_data_box_cox.head(20))\n",
    "else:\n",
    "    print(\"No columns with only positive values for Box-Cox transformation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Construction\n",
    "\n",
    "Feature construction involves creating new features from existing ones to enhance the predictive power of your models. This can be done using domain knowledge, statistical relationships between features, and various encoding techniques. In this section, we will explore various methods of feature construction and demonstrate how to apply them using practical examples.\n",
    "\n",
    "### Why Construct New Features?\n",
    "\n",
    "Constructing new features can help in:\n",
    "- **Improving Model Performance**: New features can provide additional information that helps the model make better predictions.\n",
    "- **Capturing Non-Linear Relationships**: Interactions and transformations of existing features can capture non-linear relationships.\n",
    "- **Reducing Dimensionality**: Aggregating features can reduce the number of dimensions, making models simpler and faster.\n",
    "\n",
    "### Techniques Covered:\n",
    "\n",
    "1. **Use of Domain Knowledge**: Incorporate insights from the specific field or industry to construct meaningful features.\n",
    "2. **Using Statistical Relationships Between Features**: Identify and utilize correlations and interactions between features.\n",
    "3. **Numerical Coding of Nominal Values**:\n",
    "   - **One-Hot Encoding**: Convert categorical variables into a series of binary variables.\n",
    "   - **Ordinal or Label Encoding**: Assign integer values to categories based on their order or labels.\n",
    "   - **Probability Ratio Encoding**: Encode categorical features based on the probability ratio of the target variable.\n",
    "\n",
    "\n",
    "Let's explore and apply various feature construction techniques to our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Use of Domain Knowledge\n",
    "\n",
    "Incorporating domain knowledge can help create meaningful features that enhance model performance. For our loan dataset, we can create several new features based on common financial metrics used to evaluate a borrower's creditworthiness.\n",
    "\n",
    "### New Features:\n",
    "\n",
    "1. **Debt-to-Income Ratio**: A metric to evaluate a borrower's ability to manage monthly payments and repay debts.\n",
    "\n",
    "\n",
    "   $$\n",
    "   \\text{Debt-to-Income Ratio} = \\frac{\\text{creddebt} + \\text{othdebt}}{\\text{income}}\n",
    "   $$\n",
    "   \n",
    "\n",
    "2. **Income per Year of Employment**: Indicates the stability and substantiality of a person's income relative to their employment duration.\n",
    "\n",
    "\n",
    "   $$\n",
    "   \\text{Income per Year of Employment} = \\frac{\\text{income}}{\\text{employ} + 1}\n",
    "   $$\n",
    "\n",
    "\n",
    "3. **Credit-to-Income Ratio**: Assesses how much of a person's income is used to pay off credit debt.\n",
    "\n",
    "\n",
    "   $$\n",
    "   \\text{Credit-to-Income Ratio} = \\frac{\\text{creddebt}}{\\text{income}}\n",
    "   $$\n",
    "\n",
    "\n",
    "4. **Total Debt**: Sum of all debts to gauge the total debt burden.\n",
    "\n",
    "\n",
    "   $$\n",
    "   \\text{Total Debt} = \\text{creddebt} + \\text{othdebt}\n",
    "   $$\n",
    "\n",
    "\n",
    "5. **Monthly Debt Payment Burden**: Evaluates the monthly debt payment burden relative to monthly income.\n",
    "\n",
    "\n",
    "   $$\n",
    "   \\text{Monthly Debt Payment Burden} = \\frac{\\text{Total Debt}}{\\text{income} / 12}\n",
    "   $$\n",
    "\n",
    "Let's create these new features in our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:28:48.612913Z",
     "iopub.status.busy": "2024-05-31T09:28:48.612509Z",
     "iopub.status.idle": "2024-05-31T09:28:48.638701Z",
     "shell.execute_reply": "2024-05-31T09:28:48.63741Z",
     "shell.execute_reply.started": "2024-05-31T09:28:48.612881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Make a copy of the original dataset\n",
    "data_domain_features = data.copy()\n",
    "\n",
    "# 1. Debt-to-Income Ratio\n",
    "data_domain_features['debt_to_income_ratio'] = (data_domain_features['creddebt'] + data_domain_features['othdebt']) / data_domain_features['income']\n",
    "\n",
    "# 2. Income per Year of Employment\n",
    "data_domain_features['income_per_year_employ'] = data_domain_features['income'] / (data_domain_features['employ'] + 1)  # +1 to avoid division by zero\n",
    "\n",
    "# 3. Credit-to-Income Ratio\n",
    "data_domain_features['credit_to_income_ratio'] = data_domain_features['creddebt'] / data_domain_features['income']\n",
    "\n",
    "# 4. Total Debt\n",
    "data_domain_features['total_debt'] = data_domain_features['creddebt'] + data_domain_features['othdebt']\n",
    "\n",
    "# 5. Monthly Debt Payment Burden\n",
    "data_domain_features['monthly_debt_payment_burden'] = data_domain_features['total_debt'] / (data_domain_features['income'] / 12)\n",
    "\n",
    "# Display the first 20 rows to verify the new features\n",
    "data_domain_features[['debt_to_income_ratio', 'income_per_year_employ', 'credit_to_income_ratio', 'total_debt', 'monthly_debt_payment_burden']].head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Using Statistical Relationships\n",
    "\n",
    "Utilizing statistical relationships between features can help create new features that capture interactions and dependencies within the data. This can enhance the predictive power of your models by providing additional insights and improving their ability to detect patterns.\n",
    "\n",
    "### New Features:\n",
    "\n",
    "1. **Interaction Terms**: Capturing the interaction between two or more features can help in understanding the combined effect of these features.\n",
    "2. **Polynomial Features**: Creating polynomial features can help in capturing non-linear relationships.\n",
    "3. **Ratios and Differences**: Creating features based on ratios and differences between existing features.\n",
    "\n",
    "Let's create these new features in our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:28:48.640495Z",
     "iopub.status.busy": "2024-05-31T09:28:48.640105Z",
     "iopub.status.idle": "2024-05-31T09:28:48.826428Z",
     "shell.execute_reply": "2024-05-31T09:28:48.825333Z",
     "shell.execute_reply.started": "2024-05-31T09:28:48.640459Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "# Make a copy of the original dataset\n",
    "data_stat_features = data.copy()\n",
    "\n",
    "# Impute missing values with the mean for 'age' and 'income' \n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data_stat_features[['age', 'income']] = imputer.fit_transform(data_stat_features[['age', 'income']])\n",
    "\n",
    "# 1. Interaction Terms\n",
    "# Create interaction terms between 'income' and 'debtinc'\n",
    "data_stat_features['income_debtinc_interaction'] = data_stat_features['income'] * data_stat_features['debtinc']\n",
    "\n",
    "# 2. Polynomial Features\n",
    "# Create polynomial features for 'age' and 'income'\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_features = poly.fit_transform(data_stat_features[['age', 'income']])\n",
    "poly_feature_names = poly.get_feature_names_out(['age', 'income'])\n",
    "poly_df = pd.DataFrame(poly_features, columns=poly_feature_names, index=data_stat_features.index)\n",
    "data_stat_features = pd.concat([data_stat_features, poly_df], axis=1)\n",
    "\n",
    "# 3. Ratios and Differences\n",
    "# Create ratio and difference features between 'creddebt' and 'othdebt'\n",
    "data_stat_features['creddebt_othdebt_ratio'] = data_stat_features['creddebt'] / data_stat_features['othdebt']\n",
    "data_stat_features['creddebt_othdebt_diff'] = data_stat_features['creddebt'] - data_stat_features['othdebt']\n",
    "\n",
    "# Display the first 20 rows to verify the new features\n",
    "data_stat_features[['income_debtinc_interaction', 'age', 'income', 'age^2', 'age income', 'income^2', 'creddebt_othdebt_ratio', 'creddebt_othdebt_diff']].head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Numerical Coding of Nominal Values\n",
    "\n",
    "Numerical coding of nominal values is a method to convert categorical data into numerical format, which can be easily used by machine learning models. This process is crucial when dealing with categorical features as many algorithms cannot handle non-numeric data directly.\n",
    "\n",
    "### Techniques Covered:\n",
    "\n",
    "1. **One-Hot Encoding**: Convert categorical variables into a series of binary variables.\n",
    "2. **Ordinal or Label Encoding**: Assign integer values to categories based on their order or labels.\n",
    "3. **Probability Ratio Encoding**: Encode categorical features based on the probability ratio of the target variable.\n",
    "\n",
    "Let's apply these techniques to our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3.1 One-Hot Encoding\n",
    "\n",
    "One-Hot Encoding is a technique used to convert categorical variables into a series of binary variables (0 or 1). Each category in the original variable is represented as a separate column, with a 1 indicating the presence of that category and a 0 indicating its absence. This technique is useful when there is no ordinal relationship between the categories.\n",
    "\n",
    "### Example\n",
    "\n",
    "For our dataset, we'll apply one-hot encoding to the `ed` and `default` columns, which represents education levels.\n",
    "\n",
    "Let's apply one-hot encoding to our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:28:48.827987Z",
     "iopub.status.busy": "2024-05-31T09:28:48.827631Z",
     "iopub.status.idle": "2024-05-31T09:28:48.866476Z",
     "shell.execute_reply": "2024-05-31T09:28:48.865352Z",
     "shell.execute_reply.started": "2024-05-31T09:28:48.827957Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Make a copy of the original dataset\n",
    "data_one_hot_encoded = data.copy()\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')  # drop='first' to avoid multicollinearity\n",
    "\n",
    "# Select the columns to encode\n",
    "columns_to_encode = ['ed', 'default']\n",
    "\n",
    "# Fit and transform the encoder on the selected columns\n",
    "encoded_data = encoder.fit_transform(data_one_hot_encoded[columns_to_encode])\n",
    "\n",
    "# Create a DataFrame with the encoded data\n",
    "encoded_columns = encoder.get_feature_names_out(columns_to_encode)\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoded_columns, index=data_one_hot_encoded.index)\n",
    "\n",
    "# Concatenate the encoded columns with the original dataset (excluding the original categorical columns)\n",
    "data_one_hot_encoded = pd.concat([data_one_hot_encoded.drop(columns_to_encode, axis=1), encoded_df], axis=1)\n",
    "\n",
    "# Display the first 20 rows to verify the one-hot encoding\n",
    "data_one_hot_encoded.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3.2 Ordinal or Label Encoding\n",
    "\n",
    "Ordinal or Label Encoding is a technique used to convert categorical variables into integer values based on their order or labels. This technique is useful when there is an ordinal relationship between the categories, or when simply converting categorical data into a numerical format.\n",
    "\n",
    "### Example\n",
    "\n",
    "For our dataset, we'll apply label encoding to the `ed` column, which represents education levels, and the `default` column, which represents default status.\n",
    "\n",
    "Let's apply label encoding to our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:28:48.868293Z",
     "iopub.status.busy": "2024-05-31T09:28:48.867949Z",
     "iopub.status.idle": "2024-05-31T09:28:48.883857Z",
     "shell.execute_reply": "2024-05-31T09:28:48.882719Z",
     "shell.execute_reply.started": "2024-05-31T09:28:48.868265Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Make a copy of the original dataset\n",
    "data_label_encoded = data.copy()\n",
    "\n",
    "# Initialize the LabelEncoder for 'ed' and 'default'\n",
    "label_encoder_ed = LabelEncoder()\n",
    "label_encoder_default = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'ed' column\n",
    "data_label_encoded['ed'] = label_encoder_ed.fit_transform(data_label_encoded['ed'])\n",
    "\n",
    "# Fit and transform the 'default' column\n",
    "data_label_encoded['default'] = label_encoder_default.fit_transform(data_label_encoded['default'])\n",
    "\n",
    "# Display the first 20 rows to verify the label encoding\n",
    "data_label_encoded[['ed', 'default']].head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3.3 Probability Ratio Encoding\n",
    "\n",
    "Probability Ratio Encoding is a technique that encodes categorical features based on the probability ratio of the target variable for each category. This method is particularly useful when the categories have a predictive relationship with the target variable.\n",
    "\n",
    "### Example\n",
    "\n",
    "For our dataset, we'll apply probability ratio encoding to the `ed` column, which represents education levels, using the `default` column as the target variable.\n",
    "\n",
    "Let's apply probability ratio encoding to our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:28:48.885542Z",
     "iopub.status.busy": "2024-05-31T09:28:48.885174Z",
     "iopub.status.idle": "2024-05-31T09:28:49.00819Z",
     "shell.execute_reply": "2024-05-31T09:28:49.006971Z",
     "shell.execute_reply.started": "2024-05-31T09:28:48.885504Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "\n",
    "# Make a copy of the original dataset\n",
    "data_prob_ratio_encoded = data.copy()\n",
    "\n",
    "# Initialize the TargetEncoder (Probability Ratio Encoder)\n",
    "prob_ratio_encoder = ce.TargetEncoder(cols=['ed'])\n",
    "\n",
    "# Fit the encoder to the data and transform the 'ed' column\n",
    "data_prob_ratio_encoded['ed_prob_ratio'] = prob_ratio_encoder.fit_transform(data_prob_ratio_encoded['ed'], data_prob_ratio_encoded['default'])\n",
    "\n",
    "# Display the first 20 rows to verify the probability ratio encoding\n",
    "data_prob_ratio_encoded[['ed', 'ed_prob_ratio']].head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Discretization\n",
    "\n",
    "Feature discretization involves transforming continuous features into discrete ones. This process can simplify models, capture non-linear relationships, and improve interpretability. In this section, we will explore various discretization techniques and demonstrate how to apply them using practical examples.\n",
    "\n",
    "### Techniques Covered:\n",
    "\n",
    "1. **Domain Knowledge**: Use expert knowledge to define meaningful bins.\n",
    "2. **Equal-Width Binning**: Divide the range of values into equal-width bins.\n",
    "3. **Equal-Frequency Binning**: Divide the range of values so that each bin has approximately the same number of observations.\n",
    "4. **K-Means Binning**: Use k-means clustering to create bins based on feature similarity.\n",
    "5. **ChiMerge**: Merge bins based on the chi-squared statistic to ensure similarity with respect to the target variable.\n",
    "6. **Decision Tree Binning**: Use decision trees to create bins based on target variable splits.\n",
    "\n",
    "Let's apply these discretization techniques to our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Domain Knowledge\n",
    "\n",
    "Using domain knowledge for feature discretization involves leveraging expert insights to define meaningful bins. This method ensures that the bins are relevant and can capture important patterns in the data.\n",
    "\n",
    "\n",
    "For our dataset, we'll apply domain knowledge to discretize the following columns:\n",
    "- `income`\n",
    "- `age`\n",
    "\n",
    "Let's apply domain knowledge-based discretization to these columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:28:49.010292Z",
     "iopub.status.busy": "2024-05-31T09:28:49.009833Z",
     "iopub.status.idle": "2024-05-31T09:28:49.032817Z",
     "shell.execute_reply": "2024-05-31T09:28:49.031664Z",
     "shell.execute_reply.started": "2024-05-31T09:28:49.010262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Make a copy of the original dataset\n",
    "data_discretized = data.copy()\n",
    "\n",
    "# Define bins for 'income' based on domain knowledge\n",
    "# Example bins: Low (< 30k), Medium (30k-60k), High (60k-100k), Very High (> 100k)\n",
    "income_bins = [0, 30000, 60000, 100000, float('inf')]\n",
    "income_labels = [1, 2, 3, 4]  # Using numbers for labels\n",
    "data_discretized['income_bin_domain'] = pd.cut(data_discretized['income'], bins=income_bins, labels=income_labels)\n",
    "\n",
    "# Define bins for 'age' based on domain knowledge\n",
    "# Example bins: Young (< 25), Adult (25-40), Middle-Aged (40-60), Senior (> 60)\n",
    "age_bins = [0, 25, 40, 60, float('inf')]\n",
    "age_labels = [1, 2, 3, 4]  # Using numbers for labels\n",
    "data_discretized['age_bin_domain'] = pd.cut(data_discretized['age'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# Display the first 20 rows to verify the domain knowledge-based discretization\n",
    "data_discretized[['income', 'income_bin_domain', 'age', 'age_bin_domain']].head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Equal-Width Binning\n",
    "\n",
    "Equal-Width Binning divides the range of values into equal-width bins. This technique is useful for creating evenly spaced intervals, which can help in simplifying the model and capturing important patterns.\n",
    "\n",
    "\n",
    "For our dataset, we'll apply equal-width binning to the following columns:\n",
    "- `income`\n",
    "- `debtinc`\n",
    "\n",
    "We'll use the `KBinsDiscretizer` from `sklearn.preprocessing` to perform the equal-width binning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:39:59.743086Z",
     "iopub.status.busy": "2024-05-31T09:39:59.742672Z",
     "iopub.status.idle": "2024-05-31T09:39:59.767919Z",
     "shell.execute_reply": "2024-05-31T09:39:59.766813Z",
     "shell.execute_reply.started": "2024-05-31T09:39:59.743054Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import pandas as pd\n",
    "\n",
    "# Make a copy of the original dataset\n",
    "data_discretized = data.copy()\n",
    "\n",
    "# Initialize the KBinsDiscretizer for equal-width binning\n",
    "# Strategy 'uniform' is used for equal-width binning\n",
    "kbin_discretizer = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='uniform')\n",
    "\n",
    "# Select the columns to discretize\n",
    "columns_to_discretize = ['income', 'age']\n",
    "\n",
    "# Fit the discretizer to the data and transform the selected columns\n",
    "data_discretized[columns_to_discretize] = kbin_discretizer.fit_transform(data_discretized[columns_to_discretize])\n",
    "\n",
    "# Display the first 20 rows to verify the equal-width binning\n",
    "data_discretized[['income', 'age']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Equal-Frequency Binning\n",
    "\n",
    "Equal-Frequency Binning divides the range of values so that each bin has approximately the same number of observations. This technique is useful for creating bins with an equal number of data points.\n",
    "\n",
    "\n",
    "Let's apply equal-frequency binning to the `income` and `age` columns.\n",
    "\n",
    "We'll use the `KBinsDiscretizer` from `sklearn.preprocessing` to perform the equal-frequency binning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:40:16.469197Z",
     "iopub.status.busy": "2024-05-31T09:40:16.467817Z",
     "iopub.status.idle": "2024-05-31T09:40:16.494393Z",
     "shell.execute_reply": "2024-05-31T09:40:16.492996Z",
     "shell.execute_reply.started": "2024-05-31T09:40:16.46915Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import pandas as pd\n",
    "\n",
    "# Strategy 'quantile' is used for equal-frequency binning\n",
    "kbin_discretizer = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='quantile')\n",
    "\n",
    "# Select the columns to discretize\n",
    "columns_to_discretize = ['income', 'age']\n",
    "\n",
    "# Fit the discretizer to the training data and transform both training and testing data\n",
    "train_data_discretized = train_data.copy()\n",
    "test_data_discretized = test_data.copy()\n",
    "\n",
    "train_data_discretized[columns_to_discretize] = kbin_discretizer.fit_transform(train_data[columns_to_discretize])\n",
    "test_data_discretized[columns_to_discretize] = kbin_discretizer.transform(test_data[columns_to_discretize])\n",
    "\n",
    "# Display the first 20 rows to verify the equal-frequency binning on training data\n",
    "train_data_discretized[['income', 'age']].head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 K-Means Binning\n",
    "\n",
    "K-Means Binning uses k-means clustering to create bins based on feature similarity. This technique is useful for capturing clusters within the data.\n",
    "\n",
    "\n",
    "Let's apply k-means binning to the `income` and `age` columns.\n",
    "\n",
    "We'll use the `KMeans` from `sklearn.cluster` to perform the k-means binning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:42:49.256513Z",
     "iopub.status.busy": "2024-05-31T09:42:49.255491Z",
     "iopub.status.idle": "2024-05-31T09:42:49.464867Z",
     "shell.execute_reply": "2024-05-31T09:42:49.463604Z",
     "shell.execute_reply.started": "2024-05-31T09:42:49.256474Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Initialize the KMeans model for binning\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "\n",
    "# Select the columns to discretize\n",
    "columns_to_discretize = ['income', 'age']\n",
    "\n",
    "# Fit the KMeans model to the training data and transform both training and testing data\n",
    "train_data_discretized = train_data.copy()\n",
    "test_data_discretized = test_data.copy()\n",
    "\n",
    "for column in columns_to_discretize:\n",
    "    # Reshape the data for KMeans\n",
    "    train_col_reshaped = train_data[[column]].values.reshape(-1, 1)\n",
    "    test_col_reshaped = test_data[[column]].values.reshape(-1, 1)\n",
    "    \n",
    "    # Fit the KMeans model to the training data\n",
    "    kmeans.fit(train_col_reshaped)\n",
    "    \n",
    "    # Transform the training and testing data\n",
    "    train_data_discretized[column + '_bin_kmeans'] = kmeans.predict(train_col_reshaped)\n",
    "    test_data_discretized[column + '_bin_kmeans'] = kmeans.predict(test_col_reshaped)\n",
    "\n",
    "# Display the first 20 rows to verify the k-means binning on training data\n",
    "train_data_discretized[[f'{col}_bin_kmeans' for col in columns_to_discretize]].head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 ChiMerge\n",
    "\n",
    "ChiMerge uses a chi-squared statistic to merge bins based on the similarity of the target variable distribution. This technique is useful for creating bins that are statistically similar with respect to the target variable.\n",
    "\n",
    "\n",
    "Let's apply ChiMerge to the `income` and `age` columns using the `default` target variable.\n",
    "\n",
    "We will use the `woebin` function from the `scorecardpy` package to perform the discretization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:52:55.965762Z",
     "iopub.status.busy": "2024-05-31T09:52:55.964407Z",
     "iopub.status.idle": "2024-05-31T09:53:08.757837Z",
     "shell.execute_reply": "2024-05-31T09:53:08.756238Z",
     "shell.execute_reply.started": "2024-05-31T09:52:55.965715Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install scorecardpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:55:06.902504Z",
     "iopub.status.busy": "2024-05-31T09:55:06.902033Z",
     "iopub.status.idle": "2024-05-31T09:55:10.01004Z",
     "shell.execute_reply": "2024-05-31T09:55:10.008637Z",
     "shell.execute_reply.started": "2024-05-31T09:55:06.902468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scorecardpy as sc\n",
    "\n",
    "# Assuming train_data and test_data have already been defined and split\n",
    "\n",
    "# Make copies of the training and testing datasets\n",
    "train_data_discretized = train_data.copy()\n",
    "test_data_discretized = test_data.copy()\n",
    "\n",
    "# Select the columns to discretize\n",
    "columns_to_discretize = ['income', 'age']\n",
    "\n",
    "# Apply woebin to the selected columns\n",
    "bins = sc.woebin(train_data, y='default', x=columns_to_discretize, max_num_bin=4, method='chimerge')\n",
    "\n",
    "# Function to apply bins to a dataset\n",
    "def apply_bins(data, bins):\n",
    "    for col in bins.keys():\n",
    "        bin_df = bins[col]\n",
    "        bin_edges = bin_df['breaks'].apply(lambda x: float(x.split(\",\")[0].replace(\"(\", \"\").replace(\"[\", \"\")))\n",
    "        bin_edges = [-float('inf')] + sorted(set(bin_edges)) + [float('inf')]  # Ensure unique edges\n",
    "        data[col + '_bin_chimerge'] = pd.cut(data[col], bins=bin_edges, labels=False, include_lowest=True, duplicates='drop')\n",
    "    return data\n",
    "\n",
    "# Apply the bins to the training and testing data\n",
    "train_data_discretized = apply_bins(train_data_discretized, bins)\n",
    "test_data_discretized = apply_bins(test_data_discretized, bins)\n",
    "\n",
    "# Display the first 20 rows to verify the ChiMerge binning on the training data\n",
    "train_data_discretized[[f'{col}_bin_chimerge' for col in columns_to_discretize]].head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6 Decision Tree Binning\n",
    "\n",
    "Decision Tree Binning uses a decision tree to determine binning points based on the target variable. This technique is useful for creating bins that optimize splits with respect to the target variable.\n",
    "\n",
    "\n",
    "Let's apply decision tree binning to the `income` and `age` columns using the `default` target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:59:27.280105Z",
     "iopub.status.busy": "2024-05-31T09:59:27.279252Z",
     "iopub.status.idle": "2024-05-31T09:59:27.325047Z",
     "shell.execute_reply": "2024-05-31T09:59:27.323866Z",
     "shell.execute_reply.started": "2024-05-31T09:59:27.280062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Ensure the target variable is properly encoded\n",
    "label_encoder = LabelEncoder()\n",
    "train_data['default'] = label_encoder.fit_transform(train_data['default'])\n",
    "test_data['default'] = label_encoder.transform(test_data['default'])\n",
    "\n",
    "# Make copies of the training and testing datasets\n",
    "train_data_discretized = train_data.copy()\n",
    "test_data_discretized = test_data.copy()\n",
    "\n",
    "# Select the columns to discretize\n",
    "columns_to_discretize = ['income', 'age']\n",
    "\n",
    "# Function to apply decision tree binning to a dataset\n",
    "def apply_decision_tree_binning(train_data, test_data, column, target, max_leaf_nodes):\n",
    "    # Initialize the decision tree model\n",
    "    tree_model = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes, random_state=42)\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    tree_model.fit(train_data[[column]], train_data[target])\n",
    "    \n",
    "    # Extract the binning points from the decision tree model\n",
    "    binning_points = np.sort(tree_model.tree_.threshold[tree_model.tree_.threshold != -2])\n",
    "    \n",
    "    # Ensure unique binning points and include boundaries\n",
    "    binning_points = np.unique(binning_points)\n",
    "    binning_points = [-float('inf')] + binning_points.tolist() + [float('inf')]\n",
    "    \n",
    "    # Apply the bins to the training and testing data\n",
    "    train_data[column + '_bin_tree'] = pd.cut(train_data[column], bins=binning_points, labels=False, include_lowest=True)\n",
    "    test_data[column + '_bin_tree'] = pd.cut(test_data[column], bins=binning_points, labels=False, include_lowest=True)\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "# Apply decision tree binning to the selected columns\n",
    "for column in columns_to_discretize:\n",
    "    train_data_discretized, test_data_discretized = apply_decision_tree_binning(train_data_discretized, test_data_discretized, column, 'default', max_leaf_nodes=4)\n",
    "\n",
    "# Display the first 20 rows to verify the decision tree binning on the training data\n",
    "train_data_discretized[[f'{col}_bin_tree' for col in columns_to_discretize]].head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Also you can use Feature-engine Library for Desicion tree binning and more\n",
    "https://feature-engine.trainindata.com/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank You for Exploring This Notebook!\n",
    "\n",
    "If you have any questions, suggestions, or just want to discuss any of the topics further, please don't hesitate to reach out or leave a comment. Your feedback is not only welcome but also invaluable! If you have any additional insights or methods that were not covered in this notebook, please suggest them in the comments. This notebook will be updated regularly to include more helpful tips and techniques!\n",
    "\n",
    "Happy analyzing, and stay curious!\n",
    "\n",
    "Best regards,\n",
    "\n",
    "[Matin Mahmoudi](https://www.kaggle.com/matinmahmoudi)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5115791,
     "sourceId": 8559240,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
